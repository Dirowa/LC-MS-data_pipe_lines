# LC-MS2 data dynamic pipeline

## the wrapper
The LC-MS2 wrapper is not an ordinary data-pipeline.
Due the the huge amounts of variables and options is an dynamic pipeline created with an shell based interface
With the Shell based interface is it possible to analyse datasets by remote computing
The wrapper can be initialised by python and supports the Windows and Linux OS. 

#### wrapper functions

inside the wrapper are serveral inbuild functions to improve functionality
When the wrapper is excecuted for the first time it will generate a few importand setting files which worked when testing the data pipeline
These settings can be adjusted inside the Shell system also by just using copy and paste. paths will be directly put in the right format.
The settings can also be adjusted by opening the txt file and adjust the items "note keep the same layout or the wrapper will not know anymore what to do"
When running an script from the wrapper a choise menu will be opend. users can fill in their adjustments to the Default settings or press ENTER for the default setting.
After choosing the variables it will produce a txt file in the script_settings folder with the used settings to keep a track on which settings used.
These files can also be used as import as settings to the data_pipeline when running a script.

A batch run can be used. the files required for that can be found in the Batch folder with example setting file. This can be adjusted to server your needs. 

## datasets
with the datasets is it important to have the following items in the Sample Meta data
It requires The sample names, Batch number, injection orders and sampletypes.
incase batch is unknown run serveral PCA plots todecide or fill in 1 with all samples.
The injection order is the order from oldest to youngest when data is analysed by the LC-MS
in the sampletypes are the following information required. Blanks, Qc (or pooled samples) and samples, Internstal standards are not yet incorperated into this data-pipeline.

## databases
the databases are generated by Using MetaboShiny.
Installing MetaboShiny can be quite troublesome, for that check "Tips_for_installing_metaboshiny_on_clean_linux.R" to install dependencies for a clean Linux system
Metaboshiny parsers small public databases and generetes from the SMILE formula the monoisotopic mass.
Later in the datapipeline compounds will be filterd for the monoisotopic mass
It is also possible to parse pubchem into metadobo Shiny. For this are 2 Python scripts created
1-  PubChem_parser_to import_to_extended.py can be run and the produces files have to be manually enterd in the extended.db from metaboshiny with SQLite3 commands.
2-  pubchem_parser_slow_window&linux.py does the same as PubChem_parser_to import_to_extended.py but the output files can be imported into Metaboshiny where it calculates the monoisotopic mass from the SMILES structure
